EspaÃ±ol:
    # Plataforma de Riesgo de Clientes

![Pipeline CI](https://github.com/heberth18/data-platform-cost-optimizer/actions/workflows/ci.yml/badge.svg)
![Python](https://img.shields.io/badge/python-3.10-blue)
![dbt](https://img.shields.io/badge/dbt-1.6.0-orange)
![Airflow](https://img.shields.io/badge/airflow-2.7.0-red)

> Plataforma de detecciÃ³n de fraude en tiempo real y analÃ­tica Customer 360 con validaciÃ³n automatizada de calidad de datos

---

## ğŸ“‹ Resumen

Pipeline ELT end-to-end que procesa datos de e-commerce para generar **perfiles Customer 360**, detectar fraude mediante **anÃ¡lisis de riesgo multidimensional**, y entregar inteligencia de negocio accionable a travÃ©s de dashboards automatizados.

**Capacidades Clave:**
- ğŸ” **DetecciÃ³n de fraude multidimensional** con 6 factores de riesgo (velocidad, geogrÃ¡fico, comportamental, perfil, monto, temporal)
- ğŸ‘¥ **Perfiles Customer 360** con historial completo de comportamiento y transacciones
- ğŸ›¡ï¸ **Enmascaramiento de PII** para privacidad de datos y cumplimiento normativo
- âœ… **71 tests automatizados de calidad de datos** garantizando confiabilidad del pipeline
- ğŸ“Š **3 dashboards de Metabase** para inteligencia de negocio en tiempo real
- ğŸ”„ **Pipeline CI/CD** con validaciÃ³n automatizada de calidad de cÃ³digo

---

## ğŸ—ï¸ Arquitectura
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DummyJSON API  â”‚  (External data source)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              AIRFLOW ORCHESTRATION                   â”‚
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ Extract  â”‚â†’ â”‚Transform â”‚â†’ â”‚ Validate â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                    â†“                â”‚
â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚                              â”‚   dbt    â”‚          â”‚
â”‚                              â”‚ Staging  â”‚          â”‚
â”‚                              â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                   â†“                â”‚
â”‚                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚                            â”‚dbt Tests â”‚           â”‚
â”‚                            â”‚ Staging  â”‚           â”‚
â”‚                            â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                 â†“                 â”‚
â”‚                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚                        â”‚ Fraud Detection  â”‚      â”‚
â”‚                        â”‚     Engine       â”‚      â”‚
â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                 â†“                â”‚
â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚                          â”‚   dbt    â”‚           â”‚
â”‚                          â”‚  Marts   â”‚           â”‚
â”‚                          â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                               â†“                 â”‚
â”‚                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚                        â”‚dbt Tests â”‚            â”‚
â”‚                        â”‚  Marts   â”‚            â”‚
â”‚                        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚    PostgreSQL      â”‚
                  â”‚   (3 schemas)      â”‚
                  â”‚ staging/analytics  â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚     Metabase       â”‚
                  â”‚   (3 Dashboards)   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Stack TecnolÃ³gico:**
- **OrquestaciÃ³n:** Apache Airflow 2.7.0
- **TransformaciÃ³n:** dbt 1.6.0
- **Base de Datos:** PostgreSQL 14
- **AnalÃ­tica:** Metabase
- **ContainerizaciÃ³n:** Docker Compose
- **CI/CD:** GitHub Actions
- **Lenguaje:** Python 3.10

---

## âœ¨ CaracterÃ­sticas

### 1. **Motor de DetecciÃ³n de Fraude Multidimensional**
6 algoritmos independientes de scoring de riesgo:
- **Riesgo de Velocidad:** Patrones de frecuencia de transacciones
- **Riesgo GeogrÃ¡fico:** DetecciÃ³n de anomalÃ­as de ubicaciÃ³n
- **Riesgo Comportamental:** AnÃ¡lisis de patrones de compra
- **Riesgo de Perfil:** ValidaciÃ³n de completitud de cuenta
- **Riesgo de Monto:** AnomalÃ­as en montos de transacciÃ³n
- **Riesgo Temporal:** Patrones de tiempo inusuales

**Salida:** Score de riesgo compuesto (0.0-1.0) con recomendaciones accionables

### 2. **Perfiles Customer 360**
Vista unificada del cliente con:
- Historial transaccional completo
- SegmentaciÃ³n comportamental (Premium, Regular, Nuevo)
- ClasificaciÃ³n de valor (VIP, Valor Medio, EstÃ¡ndar)
- Scoring de actividad (0-100)
- MÃ©tricas de completitud de perfil

### 3. **Seguridad de Datos**
- Enmascaramiento de PII (emails, telÃ©fonos, nombres)
- Conexiones de Airflow encriptadas (Fernet)
- GestiÃ³n de secretos basada en variables de entorno
- Sin credenciales en Git

### 4. **Aseguramiento de Calidad de Datos**
**71 tests automatizados en 3 capas:**
- **20 tests** en fuentes de datos raw
- **25 tests** en modelos staging
- **25 tests** en marts de analytics
- **1 test personalizado** (validaciÃ³n de revenue)

**Tipos de test:** unique, not_null, relationships, accepted_values, accepted_range

### 5. **Inteligencia de Negocio**
3 dashboards de Metabase:
- **Dashboard de SegmentaciÃ³n de Clientes:** AnÃ¡lisis RFM y tiers de valor
- **Dashboard de AnalÃ­tica de Fraude:** DistribuciÃ³n de riesgo y clientes de alto riesgo
- **Dashboard de Clientes de Alto Valor:** RetenciÃ³n VIP y anÃ¡lisis de revenue

---

## ğŸš€ Inicio RÃ¡pido

### Prerequisitos
- Docker & Docker Compose
- Git
- 8GB RAM mÃ­nimo

### InstalaciÃ³n

1. **Clonar repositorio**
```bash
git clone https://github.com/heberth18/data-platform-cost-optimizer.git
cd data-platform-cost-optimizer
```

2. **Crear archivo `.env`**
```bash
cp .env.example .env
```

Editar `.env` con tu configuraciÃ³n:
```env
# PostgreSQL
POSTGRES_USER=dataeng
POSTGRES_PASSWORD=dataeng123
POSTGRES_DB=data_platform

# Airflow
AIRFLOW_UID=50000
AIRFLOW__CORE__FERNET_KEY=tu_clave_fernet_aqui

# Enmascaramiento PII
ENABLE_PII_MASKING=true
```

3. **Generar clave Fernet** (para encriptaciÃ³n de Airflow)
```bash
python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
```

4. **Iniciar servicios**
```bash
docker-compose up -d
```

5. **Acceder a interfaces**
- **Airflow:** http://localhost:8080 (usuario: `airflow`, contraseÃ±a: `airflow`)
- **Metabase:** http://localhost:3000
- **PostgreSQL:** localhost:5432

---

## ğŸ“Š Uso

### Ejecutar el Pipeline

1. **Disparar DAG en UI de Airflow:**
   - Ir a http://localhost:8080
   - Habilitar `test_pipeline_4_modules`
   - Click en "Trigger DAG"

2. **Monitorear ejecuciÃ³n:**
   - Ver logs de tareas en UI de Airflow
   - Verificar resultados de calidad de datos
   - Revisar alertas de fraude

### Consultar Datos de Analytics
```sql
-- Clientes de alto riesgo
SELECT customer_id, full_name, composite_risk_score, risk_level
FROM analytics.fraud_analytics
WHERE risk_level IN ('high', 'critical')
ORDER BY composite_risk_score DESC;

-- SegmentaciÃ³n de clientes
SELECT customer_segment, COUNT(*) as clientes, AVG(total_spent) as ltv_promedio
FROM analytics.customer_segmentation
GROUP BY customer_segment;

-- Clientes VIP en riesgo
SELECT customer_id, full_name, total_spent, retention_status
FROM analytics.high_value_customers
WHERE retention_status = 'At Risk Customer';
```

MÃ¡s queries disponibles en `/queries/metabase_queries.sql`

---

## ğŸ§ª Testing

### Ejecutar tests de dbt
```bash
# Todos los tests
docker exec -it airflow-webserver dbt test --project-dir /opt/airflow/dbt

# Solo staging
docker exec -it airflow-webserver dbt test --select staging --project-dir /opt/airflow/dbt

# Solo marts
docker exec -it airflow-webserver dbt test --select marts --project-dir /opt/airflow/dbt
```

### Pipeline CI/CD
Automatizado en cada push:
- âœ… Linting de Python (Black, Flake8)
- âœ… Linting de SQL (SQLFluff)
- âœ… ValidaciÃ³n de compilaciÃ³n de dbt

---

## ğŸ“ Estructura del Proyecto
```
.
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/                # CI/CD configuration
â”‚       â””â”€â”€ ci.yml
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/                     # Airflow DAGs
â”‚   â”‚   â””â”€â”€ test_pipeline_4_modules.py
â”‚   â””â”€â”€ include/
â”‚       â”œâ”€â”€ customer_risk_platform/
â”‚       â”‚   â”œâ”€â”€ extractors.py     # API data extraction
â”‚       â”‚   â”œâ”€â”€ validators.py     # Data quality validation
â”‚       â”‚   â”œâ”€â”€ transformers.py   # Data transformation
â”‚       â”‚   â”œâ”€â”€ fraud_analyzers.py# Fraud detection engine
â”‚       â”‚   â””â”€â”€ monitoring.py     # Performance tracking
â”‚       â””â”€â”€ security/
â”‚           â””â”€â”€ pii_masking.py    # PII protection
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/              # Cleaned data models
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_customer_profiles.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_orders.sql
â”‚   â”‚   â”‚   â””â”€â”€ stg_customer_metrics.sql
â”‚   â”‚   â””â”€â”€ marts/                # Business logic models
â”‚   â”‚       â”œâ”€â”€ customer_segmentation.sql
â”‚   â”‚       â”œâ”€â”€ fraud_analytics.sql
â”‚   â”‚       â””â”€â”€ high_value_customers.sql
â”‚   â””â”€â”€ tests/                    # Custom dbt tests
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ airflow/                  # Airflow Docker config
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ init_db.sql              # Database initialization
â”‚   â””â”€â”€ setup_connections.py     # Airflow connections
â”œâ”€â”€ queries/                      # Example SQL queries
â”œâ”€â”€ docker-compose.yml           # Service orchestration
â”œâ”€â”€ makefile                     # Development commands
â””â”€â”€ requirements.txt             # Python dependencies
```

---

## ğŸ”’ Seguridad

- **Enmascaramiento de PII:** Hashing de emails (SHA-256) y enmascaramiento de campos
- **GestiÃ³n de Secretos:** Todas las credenciales en `.env` (excluido de Git)
- **EncriptaciÃ³n:** Conexiones de Airflow encriptadas con Fernet
- **Control de Acceso:** Permisos de usuario de PostgreSQL
- **.gitignore:** Exclusiones comprehensivas para datos sensibles

---

## ğŸ› ï¸ Desarrollo

### Comandos del Makefile
```bash
make setup      # Setup inicial del proyecto
make build      # Construir imÃ¡genes Docker
make up         # Iniciar todos los servicios
make down       # Detener servicios
make logs       # Ver logs
make test       # Ejecutar tests de calidad de datos
```

### Agregar Nuevos Modelos
1. Crear archivo SQL en `dbt/models/`
2. Agregar tests en `schema.yml`
3. Ejecutar `dbt run` y `dbt test`

---

## ğŸ—ºï¸ Roadmap

### En Progreso
- [ ] Deployment en GCP (BigQuery + Cloud Composer)
- [ ] Dashboards de Looker Studio
- [ ] IntegraciÃ³n con Great Expectations

---

## ğŸ“ˆ Performance

**MÃ©tricas del Pipeline:**
- **Tiempo de procesamiento:** ~1 minutos 8 segundos end to end
- **Registros procesados:** 208 clientes, 198 pedidos
- **Cobertura de pruebas:** 71 pruebas automatizadas de calidad de datos

---

## ğŸ¤ Contribuciones

Este es un proyecto de portafolio. Sugerencias y feedback bienvenidos vÃ­a issues.

---

## ğŸ“„ Licencia

Licencia MIT - Ver archivo LICENSE para detalles

---

## ğŸ‘¤ Autor

**Heberth** - [GitHub](https://github.com/heberth18)

---

English:

# Customer Risk Platform

![CI Pipeline](https://github.com/heberth18/data-platform-cost-optimizer/actions/workflows/ci.yml/badge.svg)
![Python](https://img.shields.io/badge/python-3.10-blue)
![dbt](https://img.shields.io/badge/dbt-1.6.0-orange)
![Airflow](https://img.shields.io/badge/airflow-2.7.0-red)

> Real-time fraud detection and Customer 360 analytics platform with automated data quality checks

---

## ğŸ“‹ Overview

End-to-end ELT pipeline that processes e-commerce data to generate **Customer 360 profiles**, detect fraud through **multi-dimensional risk analysis**, and deliver actionable business intelligence through automated dashboards.

**Key Capabilities:**
- ğŸ” **Multi-dimensional fraud detection** with 6 risk factors (velocity, geographic, behavioral, profile, amount, temporal)
- ğŸ‘¥ **Customer 360 profiles** with complete behavioral and transactional history
- ğŸ›¡ï¸ **PII masking** for data privacy and security compliance
- âœ… **71 automated data quality tests** ensuring pipeline reliability
- ğŸ“Š **3 Metabase dashboards** for real-time business intelligence
- ğŸ”„ **CI/CD pipeline** with automated code quality checks

---

## ğŸ—ï¸ Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DummyJSON API  â”‚  (External data source)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              AIRFLOW ORCHESTRATION                   â”‚
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ Extract  â”‚â†’ â”‚Transform â”‚â†’ â”‚ Validate â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                    â†“                â”‚
â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚                              â”‚   dbt    â”‚          â”‚
â”‚                              â”‚ Staging  â”‚          â”‚
â”‚                              â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                   â†“                â”‚
â”‚                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚                            â”‚dbt Tests â”‚           â”‚
â”‚                            â”‚ Staging  â”‚           â”‚
â”‚                            â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                 â†“                 â”‚
â”‚                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚                        â”‚ Fraud Detection  â”‚      â”‚
â”‚                        â”‚     Engine       â”‚      â”‚
â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                 â†“                â”‚
â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚                          â”‚   dbt    â”‚           â”‚
â”‚                          â”‚  Marts   â”‚           â”‚
â”‚                          â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                               â†“                 â”‚
â”‚                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚                        â”‚dbt Tests â”‚            â”‚
â”‚                        â”‚  Marts   â”‚            â”‚
â”‚                        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚    PostgreSQL      â”‚
                  â”‚   (3 schemas)      â”‚
                  â”‚ staging/analytics  â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚     Metabase       â”‚
                  â”‚   (3 Dashboards)   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Technology Stack:**
- **Orchestration:** Apache Airflow 2.7.0
- **Transformation:** dbt 1.6.0
- **Database:** PostgreSQL 14
- **Analytics:** Metabase
- **Containerization:** Docker Compose
- **CI/CD:** GitHub Actions
- **Language:** Python 3.10

---

## âœ¨ Features

### 1. **Multi-Dimensional Fraud Detection Engine**
6 independent risk scoring algorithms:
- **Velocity Risk:** Transaction frequency patterns
- **Geographic Risk:** Location anomaly detection  
- **Behavioral Risk:** Purchase pattern analysis
- **Profile Risk:** Account completeness validation
- **Amount Risk:** Transaction amount anomalies
- **Temporal Risk:** Unusual timing patterns

**Output:** Composite risk score (0.0-1.0) with actionable recommendations

### 2. **Customer 360 Profiles**
Unified customer view with:
- Complete transactional history
- Behavioral segmentation (Premium, Regular, New)
- Value tier classification (VIP, Medium Value, Standard)
- Activity scoring (0-100)
- Profile completeness metrics

### 3. **Data Security**
- PII masking (emails, phones, names)
- Encrypted Airflow connections (Fernet)
- Environment-based secrets management
- No credentials in Git

### 4. **Data Quality Assurance**
**71 automated tests across 3 layers:**
- **20 tests** on raw data sources
- **25 tests** on staging models
- **25 tests** on analytics marts
- **1 custom test** (revenue validation)

**Test types:** unique, not_null, relationships, accepted_values, accepted_range

### 5. **Business Intelligence**
3 Metabase dashboards:
- **Customer Segmentation Dashboard:** RFM analysis and value tiers
- **Fraud Analytics Dashboard:** Risk distribution and high-risk customers
- **High-Value Customer Dashboard:** VIP retention and revenue analysis

---

## ğŸš€ Quick Start

### Prerequisites
- Docker & Docker Compose
- Git
- 8GB RAM minimum

### Installation

1. **Clone repository**
```bash
git clone https://github.com/heberth18/data-platform-cost-optimizer.git
cd data-platform-cost-optimizer
```

2. **Create `.env` file**
```bash
cp .env.example .env
```

Edit `.env` with your configuration:
```env
# PostgreSQL
POSTGRES_USER=dataeng
POSTGRES_PASSWORD=dataeng123
POSTGRES_DB=data_platform

# Airflow
AIRFLOW_UID=50000
AIRFLOW__CORE__FERNET_KEY=your_fernet_key_here

# PII Masking
ENABLE_PII_MASKING=true
```

3. **Generate Fernet key** (for Airflow encryption)
```bash
python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
```

4. **Start services**
```bash
docker-compose up -d
```

5. **Access interfaces**
- **Airflow:** http://localhost:8080 (user: `airflow`, pass: `airflow`)
- **Metabase:** http://localhost:3000
- **PostgreSQL:** localhost:5432

---

## ğŸ“Š Usage

### Run the Pipeline

1. **Trigger DAG in Airflow UI:**
   - Go to http://localhost:8080
   - Enable `test_pipeline_4_modules`
   - Click "Trigger DAG"

2. **Monitor execution:**
   - View task logs in Airflow UI
   - Check data quality results
   - Review fraud alerts

### Query Analytics Data
```sql
-- High-risk customers
SELECT customer_id, full_name, composite_risk_score, risk_level
FROM analytics.fraud_analytics
WHERE risk_level IN ('high', 'critical')
ORDER BY composite_risk_score DESC;

-- Customer segmentation
SELECT customer_segment, COUNT(*) as customers, AVG(total_spent) as avg_ltv
FROM analytics.customer_segmentation
GROUP BY customer_segment;

-- VIP customers at risk
SELECT customer_id, full_name, total_spent, retention_status
FROM analytics.high_value_customers
WHERE retention_status = 'At Risk Customer';
```

More queries available in `/queries/metabase_queries.sql`

---

## ğŸ§ª Testing

### Run dbt tests
```bash
# All tests
docker exec -it airflow-webserver dbt test --project-dir /opt/airflow/dbt

# Staging only
docker exec -it airflow-webserver dbt test --select staging --project-dir /opt/airflow/dbt

# Marts only
docker exec -it airflow-webserver dbt test --select marts --project-dir /opt/airflow/dbt
```

### CI/CD Pipeline
Automated on every push:
- âœ… Python linting (Black, Flake8)
- âœ… SQL linting (SQLFluff)  
- âœ… dbt compilation checks

---

## ğŸ“ Project Structure
```
.
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/                # CI/CD configuration
â”‚       â””â”€â”€ ci.yml
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/                     # Airflow DAGs
â”‚   â”‚   â””â”€â”€ test_pipeline_4_modules.py
â”‚   â””â”€â”€ include/
â”‚       â”œâ”€â”€ customer_risk_platform/
â”‚       â”‚   â”œâ”€â”€ extractors.py     # API data extraction
â”‚       â”‚   â”œâ”€â”€ validators.py     # Data quality validation
â”‚       â”‚   â”œâ”€â”€ transformers.py   # Data transformation
â”‚       â”‚   â”œâ”€â”€ fraud_analyzers.py# Fraud detection engine
â”‚       â”‚   â””â”€â”€ monitoring.py     # Performance tracking
â”‚       â””â”€â”€ security/
â”‚           â””â”€â”€ pii_masking.py    # PII protection
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/              # Cleaned data models
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_customer_profiles.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_orders.sql
â”‚   â”‚   â”‚   â””â”€â”€ stg_customer_metrics.sql
â”‚   â”‚   â””â”€â”€ marts/                # Business logic models
â”‚   â”‚       â”œâ”€â”€ customer_segmentation.sql
â”‚   â”‚       â”œâ”€â”€ fraud_analytics.sql
â”‚   â”‚       â””â”€â”€ high_value_customers.sql
â”‚   â””â”€â”€ tests/                    # Custom dbt tests
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ airflow/                  # Airflow Docker config
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ init_db.sql              # Database initialization
â”‚   â””â”€â”€ setup_connections.py     # Airflow connections
â”œâ”€â”€ queries/                      # Example SQL queries
â”œâ”€â”€ docker-compose.yml           # Service orchestration
â”œâ”€â”€ makefile                     # Development commands
â””â”€â”€ requirements.txt             # Python dependencies
```

---

## ğŸ”’ Security

- **PII Masking:** Email hashing (SHA-256) and field masking
- **Secret Management:** All credentials in `.env` (excluded from Git)
- **Encryption:** Airflow connections encrypted with Fernet
- **Access Control:** PostgreSQL user permissions
- **.gitignore:** Comprehensive exclusions for sensitive data

---

## ğŸ› ï¸ Development

### Makefile Commands
```bash
make setup      # Initial project setup
make build      # Build Docker images
make up         # Start all services
make down       # Stop services
make logs       # View logs
make test       # Run data quality tests
```

### Adding New Models
1. Create SQL file in `dbt/models/`
2. Add tests in `schema.yml`
3. Run `dbt run` and `dbt test`

---

## ğŸ—ºï¸ Roadmap

### In Progress
- [ ] GCP deployment (BigQuery + Cloud Composer)
- [ ] Looker Studio dashboards
- [ ] Great Expectations integration

---

## ğŸ“ˆ Performance

**Pipeline Metrics:**
- **Processing time:** 1 minutes 8 seconds end-to-end
- **Records processed:** 208 customers, 198 orders
- **Test coverage:** 71 automated data quality tests

---

## ğŸ¤ Contributing

This is a portfolio project. Suggestions and feedback welcome via issues.

---

## ğŸ“„ License

MIT License - See LICENSE file for details

---

## ğŸ‘¤ Author

**Heberth** - [GitHub](https://github.com/heberth18)


